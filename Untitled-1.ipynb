{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNWithMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original subject_ratings\n",
      "             COMP10001  COMP10002  COMP10003  SWEN10003\n",
      "student_id                                            \n",
      "1                 2.0          2        2.0        NaN\n",
      "2                 9.0          9        1.0        NaN\n",
      "3                 8.0          8        NaN        1.0\n",
      "4                 NaN          7        7.0        7.0\n",
      "5                10.0         10       10.0        NaN\n",
      "6                 1.0          1        1.0        NaN\n",
      "7                 5.0          5        5.0        NaN\n",
      "8                 3.0          3        3.0        NaN\n",
      "imputed_df\n",
      "             COMP10001  COMP10002  COMP10003  SWEN10003\n",
      "student_id                                            \n",
      "1            2.000000          2   2.000000        4.0\n",
      "2            9.000000          9   1.000000        4.0\n",
      "3            8.000000          8   4.142857        1.0\n",
      "4            5.428571          7   7.000000        7.0\n",
      "5           10.000000         10  10.000000        4.0\n",
      "6            1.000000          1   1.000000        4.0\n",
      "7            5.000000          5   5.000000        4.0\n",
      "8            3.000000          3   3.000000        4.0\n",
      "similarity\n",
      " {'COMP10001': 0.08136541465828927, 'COMP10002': 0.08209951522176571, 'COMP10003': 0.10684583332538615}\n",
      "\n",
      "Top recommendations for user 1:  [('SWEN10003', 2.0)]\n"
     ]
    }
   ],
   "source": [
    "# Data Parsing\n",
    "def parse_subject_data(row):\n",
    "    subjects = {}\n",
    "    items = row.split(\", \")\n",
    "    for i in range(0, len(items), 2):\n",
    "        subject = items[i].split(\": \")[1]\n",
    "        enjoyment = int(items[i+1].split(\": \")[1])\n",
    "        subjects[subject] = enjoyment\n",
    "    return subjects\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('form3.csv')\n",
    "\n",
    "# Parse the subject data\n",
    "data['subjects'] = data['subjects'].apply(parse_subject_data)\n",
    "\n",
    "# Convert the parsed data into a DataFrame\n",
    "subject_ratings = pd.DataFrame(data['subjects'].tolist(), index=data['student_id'])\n",
    "print(\"original subject_ratings\\n\",subject_ratings)\n",
    "\n",
    "# Column mean imputation\n",
    "imputed_df = subject_ratings.fillna(subject_ratings.mean())\n",
    "#print(\"imputed_df\\n\",imputed_df)\n",
    "\n",
    "def get_itembased_scores(student_id, item, df, n=3):\n",
    "    \"\"\"\n",
    "    Return the predicted `student_id` rating for `item`, using 3 most similar items.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the original ratings for the current student_id\n",
    "    current_ratings = df.loc[student_id,:]\n",
    "    \n",
    "    # Get the imputed ratings for the current item\n",
    "    x = imputed_df.loc[:,item]\n",
    "    \n",
    "    # Initialise a predicted dictionary\n",
    "    similarity = {}\n",
    "    \n",
    "    # Only include items that student_id has rated\n",
    "    rated_items = [x for x in df.columns if not np.isnan(current_ratings[x])]\n",
    "    \n",
    "    # Calculate the similarity scores\n",
    "    for compare_item in rated_items:\n",
    "        y = imputed_df.loc[:, compare_item]\n",
    "        eucl_dist = np.sqrt(np.sum([(a-b)*(a-b) for a, b in zip(x, y)]))\n",
    "        similarity[compare_item] = 1/(1+eucl_dist)\n",
    "    #print(\"similarity\\n\",similarity)\n",
    "\n",
    "    # Convert `similarity` to a series, and find weights\n",
    "    similarity = pd.Series(similarity)\n",
    "    \n",
    "    # Create `top_n`: a LIST of the top n item labels to calculate the weighted predicted score\n",
    "    top_n = similarity.sort_values(ascending=False).head(n).index\n",
    "    \n",
    "    # Calculate the predicted score\n",
    "    predicted_score = (current_ratings[top_n]*similarity[top_n]).sum() / similarity[top_n].sum()\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "id = 1\n",
    "item = 'COMP10003'\n",
    "\n",
    "# Get top 3 recommendations for user `id`\n",
    "recommendations = []\n",
    "for item in subject_ratings.columns:\n",
    "    if np.isnan(subject_ratings.loc[id, item]):\n",
    "        recommendations.append((item, get_itembased_scores(id, item, subject_ratings)))\n",
    "\n",
    "recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:3]\n",
    "print(f\"\\nTop recommendations for user {id}: \", recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Based - Predicts users rating of a subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item': ['COMP10001', 'COMP10002', 'COMP10003', 'COMP10001', 'COMP10002', 'COMP10003', 'COMP10001', 'COMP10002', 'SWEN10003', 'COMP10002', 'SWEN10003', 'COMP10003', 'COMP10001', 'COMP10002', 'COMP10003', 'COMP10001', 'COMP10002', 'COMP10003', 'COMP10001', 'COMP10002', 'COMP10003', 'COMP10001', 'COMP10002', 'COMP10003'], 'user': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8], 'rating': [2, 2, 2, 9, 9, 1, 8, 8, 1, 7, 7, 7, 10, 10, 10, 1, 1, 1, 5, 5, 5, 3, 3, 3]}\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "2.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('form3.csv')\n",
    "\n",
    "# Convert input csv file from (1,\"Subject code: COMP10001, Enjoyment: 2, Subject code: COMP10002, Enjoyment: 2, Subject code: COMP10003, Enjoyment: 2\") to a different row for each subject for each student id\n",
    "ratings_dict = {\n",
    "    \"item\" : [],\n",
    "    \"user\" : [],\n",
    "    \"rating\" : []\n",
    "}\n",
    "\n",
    "# Parse the subject data\n",
    "for index, row in df.iterrows():\n",
    "    items = row[\"subjects\"].split(\", \")\n",
    "    for i in range(0, len(items), 2):\n",
    "        subject = items[i].split(\": \")[1]\n",
    "        ratings_dict['item'].append(subject)\n",
    "        rating = int(items[i+1].split(\": \")[1])\n",
    "        ratings_dict['rating'].append(rating)\n",
    "        ratings_dict['user'].append(row['student_id'])\n",
    "\n",
    "print(ratings_dict)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "# Create dataset from dataframe\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "\n",
    "# User-based cosine similarity\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": True,\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "training_set = data.build_full_trainset()\n",
    "\n",
    "algo.fit(training_set)\n",
    "\n",
    "prediction = algo.predict(7, \"SWEN10003\")\n",
    "print(prediction.est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
